classdef RegularisedLinearRegression
    properties
        X = [];
        y = [];
        Xtest = [];
        ytest = [];
        Xval = [];
        yval = [];
        
        data = [];
        m = 1;
        iterations = 1000;
        lambda = 1;
        theta = []
    end
    methods
        %Constructor
        function RLR = RegularisedLinearRegression(string)
            structure = load(string);
       
            RLR.X = structure.X;
            RLR.y = structure.y;
            RLR.Xtest = structure.Xtest;
            RLR.ytest = structure.ytest;
            RLR.Xval = structure.Xval;
            RLR.yval = structure.yval;
            
            RLR.m = size(RLR.X, 1);
            if size(RLR.X,1) ~= size(RLR.y,1)
                error('Invalid imported data');
            end
        end
        %   Plots the data points X and y into a new figure 
        %   It plots the data points with + for the positive examples
        %   and o for  negative examples. X is assumed to be a Mx2 matrix.
        function PlotData(RLR,X,y)
            % m = Number of examples
            m = size(X, 1);

            % Plot training data
            plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);
            xlabel('Change in water level (x)');
            ylabel('Water flowing out of the dam (y)');
        end
        %   In some cases, a dataset might have data points that are not
        %   linearly separable. However, you would still like to use 
        %   logistic regression to classify the data points.
        %
        %   To do so, you introduce more features to use -- in particular, 
        %   you add polynomial features to our data matrix (similar to
        %   polynomialregression).
        %   Maps the two input features to quadratic features 
        %   used in the regularization exercise.
        %   Returns a new feature array with more features, comprising of 
        %   X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..

        %   Inputs X1, X2 must be the same size
        function featured_map = MapFeature(RLR,col1,col2)
            degree = 6;
            featured_map = ones(size(RLR.X(:,1)));
            for i = 1:degree
                for j = 0:i
                    featured_map(:, end+1) = (col1.^(i-j)).*(col2.^j);
                end
            end
        end
        function init_theta = InitialiseTheta(RLR,extX)
            % Initialize fitting parameters
            init_theta = zeros(size(extX, 2), 1);
        end
        %computes the sigmoid of z(z can be a matrix,vector or scalar).
        function g = Sigmoid(RLR,z)
            g = zeros(size(z));
            g = 1./(1+ exp(-z));
        end
         
        %   Computes cost and gradient for logistic regression with 
        %   regularization Computes the cost of using theta as the 
        %   parameter for regularized logistic regression and the
        %   gradient of the cost w.r.t. to the parameters. 
        function [J,grad] = ComputeCost(RLR,X, y, theta, lambda)
            % Initialize some useful values
            m = length(y); % number of training examples

            % You need to return the following variables correctly 
            J = 0;
            grad = zeros(size(theta));

            h = X*theta;
            sum =0;


            for j=2:length(theta)
                sum = sum + theta(j)^2;
            end

            sum3 =0;
            for i=1:m
                sum3= sum3 + (h(i)-y(i))^2;
            end


            J = (1/(2*m))*sum3+ (lambda/(2*m))*sum;



            for j=1:size(theta,1)
                sum2=0;
                %j
                for i=1:m
                    sum2 = sum2 + (h(i)-y(i))*X(i,j);
                end
                if(j==1)
                    grad(j) = (1/m)*sum2;
                else
                    grad(j) = (1/m)*sum2 + lambda*theta(j)/m;
                end
            end
            
            grad = grad(:);
        end
        %   Trains linear regression given a dataset (X, y) and a
        %   regularization parameter lambda       
        %   Returns the trained parameters theta.
        function [theta] = TrainLinearReg(RLR,X, y, lambda)
            % Initialize Theta
            initial_theta = zeros(size(X, 2), 1); 

            % Create "short hand" for the cost function to be minimized
            costFunction = @(t) linearRegCostFunction(X, y, t, lambda);

            % Now, costFunction is a function that takes in only one argument
            options = optimset('MaxIter', 200, 'GradObj', 'on');

            % Minimize using fmincg
            theta = fmincg(costFunction, initial_theta, options);
        end
        
       %  Plot fit over the data
       function PlotFit(RLR, X, y, theta)
            % m = Number of examples
            m = size(X, 1);
            %  Plot fit over the data
            plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);
            xlabel('Change in water level (x)');
            ylabel('Water flowing out of the dam (y)');
            hold on;
            plot(X, [ones(m, 1) X]*theta, '--', 'LineWidth', 2)
            hold off;
       end
       
        %   Generates the train and cross validation set errors needed 
        %   to plot a learning curve
        %   Returns the train and cross validation set errors for a
        %   learning curve. In particular, it returns two vectors of 
        %   the same length- error_train and error_val.
        %   Then, error_train(i) contains the training error for
        %   i examples (and similarly for error_val(i)).
       function [error_train, error_val] = LearningCurve(RLR,X, y, Xval,...
                                            yval, lambda)
            % Number of training examples
            m = size(X, 1);

            % You need to return these values correctly
            error_train = zeros(m, 1);
            error_val   = zeros(m, 1);
        
            for i= 1:m
                theta = trainLinearReg(X(1:i,:),y(1:i),lambda);
                
                error_train(i)=computeCost(X(1:i,:),y(1:i),theta);

                Jv= computeCost(Xval,yval,theta);
                error_val(i) = Jv;
            end

            
            plot(1:m, error_train, 1:m, error_val);
            title('Learning curve for linear regression')
            legend('Train', 'Cross Validation')
            xlabel('Number of training examples')
            ylabel('Error')
            axis([0 13 0 150])

            fprintf('# Training Examples\tTrain Error\tCross Validation Error\n');
            for i = 1:m
                fprintf('  \t%d\t\t%f\t%f\n', i, error_train(i), error_val(i));
            end
       end
       
       %   Maps X (1D vector) into the p-th power
       %   It takes a data matrix X (size m x 1) and
       %   maps each example into its polynomial features where
       %   X_poly(i, :) = [X(i) X(i).^2 X(i).^3 ...  X(i).^p];
       function [X_poly] = polyFeatures(RLR,X, p)
       
            X_poly = zeros(numel(X), p);

            X_poly(:,1)= X;

            for i=2:p 
               X_poly(:,i) = X.^i; 
            end
       end 
       
       %   Normalizes features in X. 
       %   It returns a normalized version of X where
       %   the mean value of each feature is 0 and the standard deviation
       %   is 1. This is often a good preprocessing step to do when
       %   working with learning algorithms.
       function [X_norm, mu, sigma] = featureNormalize(RLR,X)
            
            mu = mean(X);
            X_norm = bsxfun(@minus, X, mu);

            sigma = std(X_norm);
            X_norm = bsxfun(@rdivide, X_norm, sigma);

       end 
       function FeatureMapping(RLR,X,Xtest,Xval)
           m = size(X, 1);

           p = 8;

           % Map X onto Polynomial Features and Normalize
           X_poly = polyFeatures(X, p);
           [X_poly, mu, sigma] = featureNormalize(X_poly);  % Normalize
           X_poly = [ones(m, 1), X_poly];                   % Add Ones

            % Map X_poly_test and normalize (using mu and sigma)
            X_poly_test = polyFeatures(Xtest, p);
            X_poly_test = bsxfun(@minus, X_poly_test, mu);
            X_poly_test = bsxfun(@rdivide, X_poly_test, sigma);
            X_poly_test = [ones(size(X_poly_test, 1), 1), X_poly_test];         % Add Ones

            % Map X_poly_val and normalize (using mu and sigma)
            X_poly_val = polyFeatures(Xval, p);
            X_poly_val = bsxfun(@minus, X_poly_val, mu);
            X_poly_val = bsxfun(@rdivide, X_poly_val, sigma);
            X_poly_val = [ones(size(X_poly_val, 1), 1), X_poly_val];           % Add Ones

            fprintf('Normalized Training Example 1:\n');
            fprintf('  %f  \n', X_poly(1, :));
           
       end
       
       %   Experiment with polynomial regression with multiple
        %  values of lambda. Try running the code with different values of
        %  lambda to see how the fit and learning curve change.
       function TestLearningCurve(RLR, X, y, yval, lambda, p)
            
            m = size(X, 1);          
            
            % Map X onto Polynomial Features and Normalize
            X_poly = polyFeatures(X, p);
            [X_poly, mu, sigma] = featureNormalize(X_poly);  % Normalize
            X_poly = [ones(m, 1), X_poly];
            
             [theta] = trainLinearReg(X_poly, y, lambda);
            
            % Map X_poly_val and normalize (using mu and sigma)
            X_poly_val = polyFeatures(RLR.Xval, p);
            X_poly_val = bsxfun(@minus, X_poly_val, mu);
            X_poly_val = bsxfun(@rdivide, X_poly_val, sigma);
            X_poly_val = [ones(size(X_poly_val, 1), 1), X_poly_val];
             
            % Plot training data and fit
            figure(1);
            plot(X, y, 'rx', 'MarkerSize', 10, 'LineWidth', 1.5);
            plotFit(min(X), max(X), mu, sigma, theta, p);
            xlabel('Change in water level (x)');
            ylabel('Water flowing out of the dam (y)');
            title (sprintf('Polynomial Regression Fit (lambda = %f)', lambda));

            figure(2);
            [error_train, error_val] = ...
                learningCurve(X_poly, y, X_poly_val, yval, lambda);
            plot(1:m, error_train, 1:m, error_val);

            title(sprintf('Polynomial Regression Learning Curve (lambda = %f)', lambda));
            xlabel('Number of training examples')
            ylabel('Error')
            axis([0 13 0 100])
            legend('Train', 'Cross Validation')

            fprintf('Polynomial Regression (lambda = %f)\n\n', lambda);
            fprintf('# Training Examples\tTrain Error\tCross Validation Error\n');
            for i = 1:m
                fprintf('  \t%d\t\t%f\t%f\n', i, error_train(i), error_val(i));
            end
       end
       %    Generates the train and validation errors needed to
       %    plot a validation curve that we can use to select lambda.
       %    Returns the train and validation errors (in error_train, error_val)
       %    for different values of lambda. 
       function ValidationCurve(RLR,X_poly, y, X_poly_val, yval);
       
           % Selected values of lambda (you should not change this)
            lambda_vec = [0 0.001 0.003 0.01 0.03 0.1 0.3 1 3 10]';

            % You need to return these variables correctly.
            error_train = zeros(length(lambda_vec), 1);
            error_val = zeros(length(lambda_vec), 1);
            
            for i = 1:length(lambda_vec)
   lambda = lambda_vec(i);
   theta = trainLinearReg(X,y,lambda);
   %[error_train, error_val] = learningCurve(X, y, Xval, yval, lambda);
    error_train(i) = computeCost(X, y, theta);
    error_val(i) = computeCost(Xval, yval, theta);
end

           
       end    
       
        %   use a built-in function (fminunc) to find the
        %   optimal parameters theta.
        function [optimal_theta, cost,lambda] = OptimalParameters(RLR,extX,initial_theta,lambda)
            %  Set options for fminunc
            options = optimset('GradObj', 'on', 'MaxIter', 400);

            %  Run fminunc to obtain the optimal theta
            %  This function will return theta and the cost 
            [optimal_theta, cost, exit_flag] = ...
            fminunc(@(t)(costFunctionReg(t, extX, RLR.y, lambda)), initial_theta, options);
        end
        %Plot boundary
        function PlotBoundary(RLR,extX,opt_theta,lambda)
            % Plot Data
            RLR.PlotData(RLR.X);
            hold on

            if size(extX, 2) <= 3
                % Only need 2 points to define a line, so choose two endpoints
                plot_x = [min(extX(:,2))-2,  max(extX(:,2))+2];

                % Calculate the decision boundary line
                plot_y = (-1./opt_theta(3)).*(opt_theta(2).*plot_x + opt_theta(1));

                % Plot, and adjust axes for better viewing
                plot(plot_x, plot_y)

                % Legend, specific for the exercise
                legend('Admitted', 'Not admitted', 'Decision Boundary')
                axis([30, 100, 30, 100])
            else
                % Here is the grid range
                u = linspace(-1, 1.5, 50);
                v = linspace(-1, 1.5, 50);

                z = zeros(length(u), length(v));
                % Evaluate z = theta*x over the grid
                for i = 1:length(u)
                    for j = 1:length(v)
                        z(i,j) = mapFeature(u(i), v(j))*opt_theta;
                    end
                end
                z = z'; % important to transpose z before calling contour

                % Plot z = 0
                % Notice you need to specify the range [0, 0]
                contour(u, v, z, [0, 0], 'LineWidth', 2)
            end
            hold off
            
            % Put some labels 
            hold on;
            title(sprintf('lambda = %g', lambda))
            
            % Labels and Legend
            xlabel('Microchip Test 1')
            ylabel('Microchip Test 2')

            legend('y = 1', 'y = 0', 'Decision boundary')

            % Specified in plot order
            hold off;
        end
        
        %   Predict whether the label is 0 or 1 using learned logistic 
        %   regression parameters theta.
        %   It computes the predictions for X using a threshold at 0.5 (i.e.,
        %   if sigmoid(theta'*x) >= 0.5, predict 1)
        %   Compute accuracy on our training set
        function accuracy = TrainAccuracy(RLR,extX,opt_theta)
            predictions = zeros(RLR.m, 1);
            h = sigmoid(extX*opt_theta);

            for j=1:RLR.m
                if h(j)< 0.5
                    predictions(j) =0;
                else
                    predictions(j) = 1;
                end    
            end
            accuracy = mean(double(predictions == RLR.y)) * 100;
            fprintf('Train Accuracy: %f\n', accuracy);
        end
    end
end